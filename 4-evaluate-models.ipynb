{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d93ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use one thread\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" # export OMP_NUM_THREADS=1\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=1\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=1\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" # export NUMEXPR_NUM_THREADS=1\n",
    "\n",
    "# Do not use GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02792215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import inspect\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "import textwrap\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from scipy.stats import pearsonr, mode\n",
    "from sklearn.base import clone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score, f1_score, accuracy_score, mean_absolute_error, log_loss\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold, KFold\n",
    "from sklearn.svm import SVR\n",
    "from tqdm import tqdm\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4979c315",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bfcf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_INDIV_SURVEY_VARS = True\n",
    "USE_IMPORTANCE_ITEMS = False\n",
    "\n",
    "df = pd.read_csv('../research-data/processed/lak22-courseload-final-studydata.csv')\n",
    "\n",
    "ADDITIONAL_INDIV_VARS = [\n",
    "    'course_name_number', 'is_stem_course', 'is_stem_student', 'course_student_stem_match',\n",
    "     'n_satisfied_prereqs_2021_Spring', 'n_satisfied_prereqs_all_past_semesters',\n",
    "    'percent_satisfied_prereqs_2021_Spring', 'percent_satisfied_prereqs_all_past_semesters',\n",
    "    'is_non_letter_grade_course', 'student_gpa', 'student_gpa_major', \n",
    "    'tl_importance', 'me_importance', 'ps_importance', 'combined_importance', \n",
    "    'tl_manage', 'me_manage', 'ps_manage', 'cl_combined_manage'\n",
    "]\n",
    "if not USE_IMPORTANCE_ITEMS:\n",
    "    for var in ['tl_importance', 'me_importance', 'ps_importance', 'combined_importance']:\n",
    "        del df[var]\n",
    "\n",
    "if not USE_INDIV_SURVEY_VARS:\n",
    "    for var in ADDITIONAL_INDIV_VARS:\n",
    "        del df[var]\n",
    "\n",
    "# Remove string section information\n",
    "for col in ['section_num','secondary_section_number','all_section_numbers']:\n",
    "    if col in df.columns:\n",
    "        del df[col]\n",
    "        \n",
    "# Remove Labels that are not needed\n",
    "for col in ['tl2', 'tl_sensitivity', 'me_sensitivity', 'ps_sensitivity', 'cl_sensitivity',\n",
    "            'tl1_smoothed_lmm', 'me_smoothed_lmm', 'ps_smoothed_lmm', 'cl_smoothed_lmm', \n",
    "            'tl1_smoothed_student_average', 'me_smoothed_student_average', 'ps_smoothed_student_average',\n",
    "            'cl_smoothed_student_average']:\n",
    "    if col in df.columns:\n",
    "        del df[col]\n",
    "\n",
    "# Drop string columns and get dummies for string var\n",
    "df = df.set_index('course_name_number')\n",
    "df = pd.get_dummies(df, columns=['class_type']) # upper, lower division, grad\n",
    "\n",
    "# Train (CV) and holdout\n",
    "train, test = train_test_split(df, test_size=0.15, random_state=12345, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8218874",
   "metadata": {},
   "source": [
    "## CV Error Control Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8988384",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/model-results-25-control variables.p', 'rb') as f:\n",
    "    prelim = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6ef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "out = pd.concat([get_sorted_model_result_table(prelim, l) for l in LABELS])\n",
    "display(out)\n",
    "pd.set_option('display.max_rows', 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0a160",
   "metadata": {},
   "source": [
    "## CV Error KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d898788",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/model-results-25-knn.p', 'rb') as f:\n",
    "    prelim = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea65d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_rows', 500)\n",
    "out = pd.concat([get_sorted_model_result_table(prelim, l) for l in LABELS])\n",
    "display(out)\n",
    "pd.set_option('display.max_rows', 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe1574",
   "metadata": {},
   "source": [
    "## Test set error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df74fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/model-results-25-control variables.p', 'rb') as f:\n",
    "    prelim = pickle.load(f)\n",
    "\n",
    "ignore_warnings=True\n",
    "if ignore_warnings:\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "res = dict()\n",
    "for l in LABELS: res[l] = dict()    \n",
    "    \n",
    "for target in tqdm(['tl1', 'me', 'ps', 'cl_combined']):\n",
    "    for model in ['random', 'linreg', 'rf', 'xgb', 'enet', 'svm', 'nn']:\n",
    "        res[target][model] = apply_model(prelim, train.copy(), test.copy(), \n",
    "                                         target=target, model_ref=model,\n",
    "                                        imputing_strategy='control variables')\n",
    "\n",
    "# Add ensemble\n",
    "for target in tqdm(['tl1', 'me', 'ps', 'cl_combined']):\n",
    "    temp = []\n",
    "    for model in ['linreg', 'rf', 'xgb', 'enet', 'svm', 'nn']:\n",
    "        temp.append(res[target][model][0])\n",
    "    res[target]['ensemble'] = (list(map(np.mean, zip(*temp))), res[target]['linreg'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is test set error related to credit hour vs. predicted load discrepancy?\n",
    "def correlation_pairwise_complete(series1, series2, print_n=False):\n",
    "    x, y = series1.values, series2.values\n",
    "    nas = np.logical_or(np.isnan(x), np.isnan(y))\n",
    "    corr = scipy.stats.pearsonr(x[~nas], y[~nas])\n",
    "    if print_n:\n",
    "        print(f'N = {len(x[~nas])}')\n",
    "    return corr\n",
    "\n",
    "def standardize_z(v):\n",
    "    return (v - np.mean(v)) / np.std(v)\n",
    "    \n",
    "tmp = pd.DataFrame({\n",
    "    'pred': res['cl_combined']['ensemble'][0],\n",
    "    'label': res['cl_combined']['ensemble'][1],\n",
    "    'n_credit_hours': test['n_credit_hours']\n",
    "})\n",
    "tmp['mae'] = np.abs(tmp['pred'] - tmp['label'])\n",
    "tmp['pred_z'] = standardize_z(tmp['pred'])\n",
    "tmp['n_credit_hours_z'] = standardize_z(tmp['n_credit_hours'])\n",
    "tmp['pred_cred_discrepancy_z'] = tmp['pred_z'] - tmp['n_credit_hours_z']\n",
    "display(tmp.head(3))\n",
    "correlation_pairwise_complete(tmp['mae'], tmp['pred_cred_discrepancy_z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tmp['mae'], tmp['pred_cred_discrepancy_z'], 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b63c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_score(gold, preds, fun=mean_squared_error, n_iter=1000, alpha=0.05, reference=0.8):\n",
    "    n_obs = len(gold)\n",
    "    true = fun(gold, preds)\n",
    "    out = []\n",
    "    for _ in range(n_iter):\n",
    "        mask = np.random.randint(n_obs, size=n_obs)\n",
    "        try:\n",
    "            tmp = fun(np.array(gold)[mask], preds[mask])\n",
    "        except:\n",
    "            continue\n",
    "        out.append(tmp)\n",
    "    p = 1 - (sum([mae < reference for mae in out])/len(out))\n",
    "    return true, np.quantile(out, alpha/2), np.quantile(out, 1-(alpha/2)), p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate \n",
    "targets, models, maes, boot_maes = [], [], [], []\n",
    "    \n",
    "for target in tqdm(['tl1', 'me', 'ps', 'cl_combined']):\n",
    "    baseline = mean_absolute_error(res[target]['random'][0], res[target]['random'][1])\n",
    "    for model in ['random', 'linreg', 'rf', 'xgb', 'enet', 'svm', 'nn', 'ensemble']:\n",
    "        targets.append(target); models.append(model)\n",
    "        maes.append(mean_absolute_error(res[target][model][0], res[target][model][1]))\n",
    "        boot_maes.append(bootstrap_score(res[target][model][0], res[target][model][1], \n",
    "                                         fun=mean_absolute_error, n_iter=10000,\n",
    "                                         reference=baseline))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({'target': targets, 'model': models, 'mae': maes,\n",
    "                        'lower': [b[1] for b in boot_maes],\n",
    "                        'upper': [b[2] for b in boot_maes],\n",
    "                        'p': [b[3] for b in boot_maes]})\n",
    "metrics.sort_values(by=['target', 'mae'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6da189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add error reduction, percentage error, and model -> rank\n",
    "res2 = []\n",
    "for target in ['tl1', 'me', 'ps', 'cl_combined']:\n",
    "    tmp = metrics[metrics['target'] == target].copy()\n",
    "    tmp = tmp.sort_values(by='mae')\n",
    "    baseline = tmp[tmp['model'] == 'random'].mae.values[0]\n",
    "    tmp['mae_diff'] = baseline - tmp['mae']\n",
    "    tmp['mae_percent_improve_to_random'] = tmp['mae_diff'] * 100 / baseline\n",
    "    tmp['model_rank'] = list(range(1, tmp.shape[0]+1))\n",
    "    res2.append(tmp)\n",
    "ans = pd.concat(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(ans, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average model rank across constructs\n",
    "ans.groupby('model')['model_rank'].mean().sort_values().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3d463",
   "metadata": {},
   "source": [
    "# Correlation betwen MAE and discrepancy on full-coded data for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/model-results-25-control variables.p', 'rb') as f:\n",
    "    prelim = pickle.load(f)\n",
    "\n",
    "full = pd.concat([train, test])\n",
    "    \n",
    "ignore_warnings=True\n",
    "if ignore_warnings:\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "res = dict()\n",
    "for l in LABELS: res[l] = dict()    \n",
    "    \n",
    "for target in tqdm(['tl1', 'me', 'ps', 'cl_combined']):\n",
    "    for model in ['random', 'linreg', 'rf', 'xgb', 'enet', 'svm', 'nn']:\n",
    "        res[target][model] = apply_model(prelim, train.copy(), full.copy(), \n",
    "                                         target=target, model_ref=model,\n",
    "                                        imputing_strategy='control variables')\n",
    "\n",
    "# Add ensemble\n",
    "for target in tqdm(['tl1', 'me', 'ps', 'cl_combined']):\n",
    "    temp = []\n",
    "    for model in ['linreg', 'rf', 'xgb', 'enet', 'svm', 'nn']:\n",
    "        temp.append(res[target][model][0])\n",
    "    res[target]['ensemble'] = (list(map(np.mean, zip(*temp))), res[target]['linreg'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b95b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame({\n",
    "    'pred': res['cl_combined']['ensemble'][0],\n",
    "    'label': res['cl_combined']['ensemble'][1],\n",
    "    'n_credit_hours': full['n_credit_hours']\n",
    "})\n",
    "tmp['mae'] = np.abs(tmp['pred'] - tmp['label'])\n",
    "tmp['pred_z'] = standardize_z(tmp['pred'])\n",
    "tmp['n_credit_hours_z'] = standardize_z(tmp['n_credit_hours'])\n",
    "tmp['pred_cred_discrepancy_z'] = tmp['pred_z'] - tmp['n_credit_hours_z']\n",
    "\n",
    "print(tmp.shape)\n",
    "\n",
    "correlation_pairwise_complete(tmp['mae'], tmp['pred_cred_discrepancy_z'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
